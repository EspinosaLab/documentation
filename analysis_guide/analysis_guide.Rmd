---
title: "`r params$title`"
output:
  html_notebook: 
    code_folding: hide
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: yes
editor_options: 
  chunk_output_type: inline
params:
  version: 0.3
  title: Standard project analysis setup
  project: Setting up a project for computation on Rosalind 
  date: !r format(Sys.time(), '%d %B %Y')
  author:
    - Kohl Kinning
  email:
    - kohl.kinning@cuanschutz.edu # can list additional emails
  affiliation: Linda Crnic Institute for Down Syndrome, University of Colorado Anschutz Medical Campus
---

***
<font style="font-size:18pt"> Project: `r params$project` </font><br>
Date:  `r params$date`  
Report version: `r params$version`   
Author(s): `r params$author`  
`r params$email`  
`r params$affiliation`

```{r}
knitr::opts_chunk$set(class.source='fold-show')
```

# Get the data

Get the data from the sequencing provider. This is commonly Novogene or the Genomics core here on campus. We'll want to have this data archived, so download it first in to a future proof location, such as the Proton server.

# Create the project

## Build some directories

You'll have to manually create a few directories to get started as scripts used later depend on a specific directory structure. Create the following hierarchy. Be sure to vreate it in the correct data-type foler, e.g. RNASeq, PROSeq, etc.

```{class.source=NULL}
├── informative_proj_name
│   ├── analysis_mm_dd_yyyy
│   │   └── scripts
│   ├── raw_mm_dd_yyyy
```

We'll use this hypoxia project as a model for the rest of the guide guide:

```{class.source=NULL}
├── HCT116_Hypoxia_spikein
│   ├── analysis_05_06_2019
│   │   └── scripts
│   ├── raw_05_06_2019
```
# Transferring data from archive to Rosalind

To be HIPAA compliant, Rosalind is not directly accessible by end users. A jumphost is used before accessing Rosalind for computation, and a transfer server is used when moving data on and off of Rosalind. The transfer server is very limited in its capabilities. The only way to transfer files to the transfer server, and further to Rosalind is through secure file transfer protocol (sftp). You can use the command line directly, or can use a GUI based app like Filezilla. If using a GUI based client, be sure to include the sftp prefix before the server, e.g. `sftp://cubipmsftp.ucdenver.pvt`. Once the files are on the transfer server, login to Rosalind and transfer from the transfer server to Rosalind.

The transfer is not for long-term storage of data, it may be deleted at any time. There is a storage limit per user. Remove the data once it's on Rosalind!

## Command line

### Connect

To open an SFTP connection `sftp` command followed by the remote server username and sftp server domain name, `cubipmsftp.ucdenver.pvt`:

```{class.source=NULL}
$ sftp remote_username@cubipmsftp.ucdenver.pvt
```

### Navigate

Some commands are similar to the ones used is  class.source=NULL.

```{class.source=NULL}
sftp> pwd
```

```{class.source=NULL}
sftp> ls
```

```{class.source=NULL}
sftp> cd
```

### Transferring

#### Download from server

To download files on to the local machine from the server, use the `get` command. Files will be downloaded in to the folder from which you launched the sftp session.

```{class.source=NULL}
sftp> get filename.tar.gz
```

To download an entire folder, use the `-r` recursive flag.

```{class.source=NULL}
sftp> get -r remote_directory
```

#### Upload to server

To upload files on to the server from the local machine, use the `put` command. Files will be uploaded in to the present working directory from the folder from which you launched the sftp session.

```{class.source=NULL}
sftp> put filename.tar.gz
```

To download an entire folder, use the `-r` recursive flag.

```{class.source=NULL}
sftp> put -r remote_directory
```

### On to Rosalind

Rosalind has contact with the transfer server. Once the data is uploaded to the transfer server you are one step away from accessing it on Rosalind. Once logged in to Rosalind, you can access the file on the transfer server freely. The environmental variable $TRANSFER is a shortcut for accessing the transfer server. Each user has a folder named as their username on the transfer server. Simply use `cp`, `mv`, or `rsync` to transfer the desired files in to your directory on Rosalind. Remember to remove your data from the transfer server.

```{class.source=NULL}
$ rsync -arzuh $TRANSFER/samples $SHARED/Projects2/RNAseq/HCT116_Hypoxia_spikein/raw_05_06_2019
```

OR

```{class.source=NULL}
$ rsync -arzuh /gpfs/transfer/username/samples $SHARED/Projects2/RNAseq/HCT116_Hypoxia_spikein/raw_05_06_2019
```

This should be the resulting directory structure.

```{class.source=NULL}
├── HCT116_Hypoxia_spikein
│   ├── analysis_05_06_2019
│   │   └── scripts
│   ├── raw_05_06_2019
│   │   ├── H202SC19030981
│   │   └── Novogene_QC
```


# Create the sample info spreadsheet

## Sequence metadata

Much of this step is manual. You'll want to reach out to whomever sent the samples for sequencing to get this information. The goal is the link the sample name with the fastq file and to ensure the the MD5 checksums of the data from the provider match the data that we've downloaded. An important step here is to determine what the final sample name will be, the name that will be used in plots in downstream analysis. It's probably a good idea to chat with the person that generated the data to figure out what would be most intutitive/informative for them. There is a template Excel spreadsheet that you can fill in. Here is an example of what the metadata may look like:

<div class="pagedtable pagedtable-not-empty" style="opacity: 1;"><table style="visibility: hidden; position: absolute; white-space: nowrap; height: auto; width: auto;">
```{r message=FALSE, warning=FALSE}
excelTable(sample_info, defaultColWidth=200, editable=FALSE, autoColTypes=FALSE)
```
</table></div>

## MD5 checksums

Some sequencing providers will send checksums with the data so that we can ensure there was no corruption resulting from the transfer process. Perform this check after downloading the files to the archive and after transfering from the archive to Rosalind. Store the results in the spreadsheet. To create the MD5 checksums for the fastq files post-transfer, use the following command:

```{class.source=NULL}
$ for file in $(find /path/to/raw/data/*.fq.gz); do md5sum $file; done > MD5_post.txt
```

Keep track of the results in the spreadsheet. You can also check whether they match or not here.

<div class="pagedtable pagedtable-not-empty" style="opacity: 1;"><table style="visibility: hidden; position: absolute; white-space: nowrap; height: auto; width: auto;">
```{r message=FALSE, warning=FALSE}
excelTable(md5_sums, defaultColWidth=200, editable=FALSE, autoColTypes=FALSE)
```
</table></div>

You can also compare the MD5 checksums within the pre- and post-transfer MD5 files with the following awk command. As with most linux tools, no news is good news. If there is a mismatch it will be reported.

```{class.source=NULL}
$ awk -F"/" 'FNR==NR{filearray[$1]=$NF; next }!($1 in filearray){printf "%s has a different md5sum\n",$NF}' MD5_post.txt MD5_pre.txt
```

## Build the sample_locations.txt

The `sample_locations.txt` file will be used in most downstream steps. Don't mess this up! Here is where we'll link the sample names to the fastq files. You can construct this any way you like. It may be helpful to use the filepath listed in the MD5.txt file generate above. Here is an example of what the sample_locations.txt may contain. It's really a delimited file with tabs as delimeters, but the pipeline will look for `sample_locations.txt` so keep this name. Be sure that there are NO column labels.

<div class="pagedtable pagedtable-not-empty" style="opacity: 1;"><table style="visibility: hidden; position: absolute; white-space: nowrap; height: auto; width: auto;">
```{r message=FALSE, warning=FALSE}
colnames(sample_locations) <- c(" ", "  ", "  ")
excelTable(sample_locations, defaultColWidth=200, editable=FALSE, autoColTypes=FALSE)
```
</table></div>

The structure should now look like this:

```{class.source=NULL}
├── HCT116_Hypoxia_spikein
│   ├── analysis_05_06_2019
│   │   ├── sample_locations.txt
│   │   └── scripts
│   ├── raw_05_06_2019
```


# Run the analysis setup script

First you'll have to copy the pipeline script in to the /scripts folder. You'll have to open this script and edit some parameters, like read length, project name, date, etc. Sections to be edited are clearly marked in the comments of the script. Now that the sample_location.txt file and the pipeline script are in place we can run the analysis setup script. Be sure to run this script from the `analysis_mm_dd_yyyy` directory. This script won't run if the necessary arguments aren't provided. It will set-up the analysis based on the pipeline script to be run, in this case the RNASeq pipeline.

**RNAseq_pipeline_v0.65_HCT116_Hypox.sh**

and here is an example RNA-Seq pipeline script, **`RNAseq_pipeline_v0.65_HCT116_Hypox.sh`**. You'll have to open this script and edit some parameters.

```{bash}
#!/bin/bash

PIPELINE_TITLE=RNAseq_pipeline
PIPELINE_VERSION=0.6
# DATE: 05-06-2019
# AUTHOR: Matthew Galbraith
# SUMMARY: 
# This script is designed to be run once for each sample (incl. both reads for PE) and is executed by submitAll.sh once per sample with the appropriate arguments.
# Version 0.3+: Pipeline script is called from sbatch SUBMIT job; each stage is now also called by sbatch (with -W and --wrap options), rather than srun in earlier versions.
# Version 0.4+: Adds functions for catching SLURM stepd errors and checking SLURM exit codes
# Version 0.5: Activating stages 13 and 14 for HOMER makeTagDirectory and HOMER makeUCSCfile
# Version 0.6: Adding spike-in analysis
# 
# Steps to run pipeline:
# 1) In Project/ : create top-level working dirs: Project/raw_date and Project/analysis_date
# 2) In Project/analysis_date/ create sample_locations.txt (field 1 = SAMPLE_NAME; field 2 = path/to/raw_fastq_file.gz; field 3 = read1 / read2 labels)
# 3) Edit variables in top section of pipeline script and (if needed) save a project-specific version (specify path to this version as indicated below)
# 4) From Project/analysis_date/ run analysis_setup.sh as follows:
# sh path/to/analysis_setup.sh <FULL/PATH/TO/PIPELINE_SCRIPT> <ACCOUNT(ESP/HTP)> <START_AT_STAGE> <END_AT_STAGE>
# 5) Then from Project/analysis_date/scripts run submitAll_START_AT_STAGE-END_AT_STAGE.sh as follows (command can be copied from within submit script):
# sbatch submitAll_START_AT_STAGE-END_AT_STAGE.sh

##### ARGUMENTS PASSED FROM submitAll.sh - DO NOT EDIT THIS SECTION #####

EXPECTED_ARGS=6
# check if correct number of arguments are supplied from command line
if [ $# -ne $EXPECTED_ARGS ]
then
        echo "Usage: sh "$PIPELINE_TITLE"v"$PIPELINE_VERSION".sh <THIS_SAMPLE_NAME> <THIS_ANALYSIS_DIR> <THIS_USER_ACCOUNT> <START_AT_STAGE> <END_AT_STAGE> <SUBMIT_LOG>
        This should normally be called by submitAll script
        ${red}ERROR - expecting "$EXPECTED_ARGS" ARGS but "$#" were provided:${NC}
        "$@"
        "
        exit 1
fi

# global pipeline variables from command line via submitAll.sh (do not edit)
THIS_SAMPLE_NAME=$1
THIS_ANALYSIS_DIR=$2
THIS_USER_ACCOUNT=$3
START_AT_STAGE=$4
END_AT_STAGE=$5
SUBMIT_LOG=$6

##### VARIABLES IN THIS SECTION CAN BE EDITED #####
## global pipeline variables ##
        SEQ_TYPE=PE                         # <PE/SE> for paired-end or single end
        STRAND_TYPE=strand-specific-rev     # <strand-specific-fwd/strand-specific-rev/unstranded>  # use fwd for Nugen TTseq round 1 library; use rev for Illumina TruSeq stranded
        READ_LENGTH=150                     # <50/75/150>  # used to trim n+1 read if present
        PIPELINE_TYPE=RNAseq
        SPIKE_IN=both                       # <none/both/dm/ercc> external RNA spike-in details; used to select correct index(s) and gtf(s) for alignment and counting - check that correct paths are ebtered for stages 4 and 11 below
        PIPELINE_SCRIPTS_DIR=/gpfs/jespinosa/shared/Matt/PipeLinesScripts/RNAseq/stageScripts                    # THIS COULD BE IMPLEMENTED AS ARG
        QC_DIR_NAME="$THIS_ANALYSIS_DIR"/QC
        ALIGNMENT_DIRNAME=="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Alignments   # NEED TO IMPLEMENT IN STAGE 4
        COUNTS_DIRNAME="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Counts
        TRACKS_DIRNAME="$THIS_ANALYSIS_DIR"/Tracks
        # FASTQR1_FILE=see switch and check below
        # FASTQR2_FILE=see switch and check below
        COMMENTS="This version is for HCT116 Hypoxia RNA-seq data with Drosophila and ERCC spike-ins.
"     # Add comments explaining this version of the analysis

## STAGE-SPECIFIC VARIABLES TO EDIT HERE ##
# 0 PRE_FASTQC
        # uncommon options are either defaults or set in 0_PRE_FASTQC.sh

# 1 FASTQ_MCF
        # common options
        MIN_SEQ_LENGTH=30       # -l option; default = 19
        MIN_QUAL=10             # -q option; default = 10; should change for Novoseq data with binned q-scores
        MIN_ADAPTER=0           # -s option; Log2 scale for adapter minimum-length-match; default=2.2 ie ~4.5)
        MIN_PERC_OCCUR=0.25     # -t option; % occurance threshold before adapter clipping; default=0.25)
        MAX_PERC_DIFF=10        # -p option; Maximum adapter difference percentage; default=10)
        CONTAMINANTS_FASTA="$SHARED"/references/Contaminants/contaminants.fa
        #uncommon options are either defaults or set in 1_FASTQ_MCF.sh
        # see also: https://github.com/ExpressionAnalysis/ea-utils/blob/wiki/FastqMcf.md

# 2 POST_FASTQC
        # uncommon options are either defaults or set in 2_POST_FASTQC.sh

# 3 FASTQ SCREEN
        # common options
        ## SUBSET=                       #NOT YET IMPLEMENTED
        ## ALIGNER=
        ## PAIRED=YES/NO                 #NEED TO IMPLEMENT AS IF STATEMENT
        # uncommon options are either defaults or set in 3_FASTQ_SCREEN.sh

# 4 MAPPING TOPHAT2_ALIGN
        # common options
        BOWTIE_INDEX="$SHARED"/references/Homo_sapiens/UCSC/hg19/Sequence/Bowtie2Index/genome       # INDEX BASENAME FOR HUMAN ONLY
        BOWTIE_INDEX_BOTH="$SHARED"/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/hg19_dm6_ERCC.genome       # INDEX BASENAME FOR HUMAN + DROSOPHILA and ERCC SPIKE-IN
        BOWTIE_INDEX_DM6=not_yet_implemented                                                        # INDEX BASENAME FOR HUMAN + DROSOPHILA
        BOWTIE_INDEX_ERCC=not_yet_implemented                                                       # INDEX BASENAME FOR HUMAN + ERCC
        # ^ use custom indexes when using Drosophila and/or ERCC spike-ins
        # BASENAME FOR HUMAN: "$SHARED"/references/Homo_sapiens/UCSC/hg19/Sequence/Bowtie2Index/genome
        # BASENAME FOR MOUSE:
        TOPHAT_GTF="$SHARED"/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/genes.gtf           # from igenomes
        TOPHAT_GTF_BOTH="$SHARED"/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/hg19_dm6_genes.gtf           # COMBINED GTF for hg19 and dm6; no need for ERCC as no introns
        TOPHAT_GTF_DM6="$SHARED"/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/hg19_dm6_genes.gtf            # same as for BOTH
        # ^ use custom gtf when using Drosophila spike-ins
        # FOR HUMAN: "$SHARED"/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/genes.gtf         # from igenomes
        TRANSCRIPTOME_INDEX="$SHARED"/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/transcriptome_index_bt2/genes  # Should only be generated on first run
        TRANSCRIPTOME_INDEX_BOTH="$SHARED"/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/transcriptome_index_bt2/hg19_dm6_known  # COMBINED hg19 + dm6
        TRANSCRIPTOME_INDEX_DM6="$SHARED"/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/transcriptome_index_bt2/hg19_dm6_known   # same as for BOTH
        # ^ use custom index when using Drosophila spike-ins
        # ^ created using this cmd: tophat -G /gpfs/jespinosa/shared/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/hg19_dm6_genes.gtf --transcriptome-index=/gpfs/jespinosa/shared/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/transcriptome_index_bt2/hg19_dm6_known /gpfs/jespinosa/shared/Matt/Refs/Bowtie2_UCSC_hg19_dm6_ERCC/hg19_dm6_ERCC.genome
        # STD HUMAN: "$SHARED"/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/transcriptome_index_bt2/genes # Should only be generated on first run
        MATE_INNER_DIST=50      # Default is 50. Should be set based on library QC if possible. May require an initial alignment run to assess.
        MATE_STD_DEV=20         # Default is 20.
                                # Leave set for SE data but will not be used.
        # uncommon options are either defaults or set in 4_TOPHAT2_ALIGN.sh

# 5 ADD RGID
        # common options
        PLATFORM=ILLUMINA_NovaSeq6000
        DATE="$(date "+%m-%d-%Y")"
        PI=MDG
        LIBRARY="$SEQ_TYPE"_"$STRAND_TYPE"
        SEQ_CORE=Novogene
        SEQ_ID=NA
        EXPERIMENT=stranded_RNAseq
        # uncommon options are either defaults or set in 5_
        PICARD_MEM_5=8G      # java memory option
        
# 6 FILTER
        MIN_MAPQ=10
# 7 SORT
        PICARD_MEM_7=16G     # java memory option
                                                                        # need to check variables in script and move here
# 8 MARK OR REMOVE DUPLICATES
        PICARD_MEM_8=64G     # java memory options
        DUPLICATES=mark           # <mark> or <remove>
                                                                        # need to check variables in script and move here
# 9 ALIGNMENT METRICS
        PICARD_MEM_9=64
        REF_FILE=$SHARED/Matt/Refs/Picard/ucsc.hg19.fasta.gz

# 10 RSEQC
        REFSEQ_BED=$SHARED/references/Homo_sapiens/UCSC/hg19/tool_resources/rseqc/hg19_RefSeq.bed
        HOUSEKEEPING_BED=$SHARED/references/Homo_sapiens/UCSC/hg19/tool_resources/rseqc/hg19.HouseKeepingGenes.bed

# 12 HTSeq COUNT
        GTF="$SHARED"/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/genes.gtf                      # from igenomes
        DM_GTF="$SHARED"/Matt/Refs/dm6/Drosophila_melanogaster/UCSC/dm6/Annotation/Genes/genes.gtf      # from igenomes
        ERCC_GTF="$SHARED"/Matt/Refs/ERCC/ERCC92/ERCC92.gtf                                             # from https://assets.thermofisher.com/TFS-Assets/LSG/manuals/ERCC92.zip
        # ^ use additional gtfs when using Drosophila and/or ERCC spike-ins
        # STD HUMAN: $SHARED/references/Homo_sapiens/UCSC/hg19/Annotation/Genes/genes.gtf      # from igenomes
        FEATURETYPE=exon
        IDATTR=gene_id
        HT_MINAQUAL=10
        MODE=intersection-nonempty
        ORDER=pos       # For paired-end data: <name> or <pos>; Default is name
        # BUFFER=       # --max-reads-in-buffer=<number>; when paired sorted by position; default is 30000000       # NOT IMPLEMENTED YET

# 13 HOMER MAKE TAG DIR
        # Stages 11 and 12 should normally be run together to ensure same settings ie -fragLength, -tbp
        HOMER_DIRNAME="$THIS_ANALYSIS_DIR"/HOMER
        REF=hg19            # used for -genome <genome version> (required for -checkGC)
        FRAG_LENGTH_s11=given  # "none" to use default; "given" to use read length (using custom BED format instead where read length = TLEN from BAM)
                               # use given for PE data to ensure fragment lenghts are correct (= read length in custom BED file)
                               ## HOMER does not fully support visualization for paired-end RNA-seq but if using fragLength given, it will plot to fist splice junction to preserve sharp exon boundaries. 
        TAGS_PER_s11="all"      # used for -tbp INT  (default: no limit) number of tags per position to keep; "all" to use default
                            # can also be set for makeUCSCfile
        # -format {sam/bam/bed}     # can be used to force format if HOMER guesses incorrectly
        # -d <tag directory> [tag directory 2] ... (add Tag directory to new tag directory)     # use to combine tag directories eg if plotting merged data
        # -unique   (default) keep only uniquely aligned reads (secondary flag unset and MAPQ >10)
        # -tbp INT  (default: no limit) number of tags per position to keep
        # -single (Create a single tags.tsv file for all "chromosomes" - i.e. if >100 chromosomes)
        # -checkGC  check Sequence bias, requires "-genome"
        # If you use paired-end reads for ChIP-Seq or RNA-Seq, HOMER will treat each half of the read separately (and count each as 0.5 reads), which works well for a number of applications.
        # If you are using stranded paired-end reads, make sure to specify "-sspe" so that HOMER will correctly interpret the intended strand for the 2nd read in the mate-pair.

# 14 HOMER MAKE UCSC FILE
        # Stages 11 and 12 should normally be run together to ensure same settings ie -fragLength, -tbp
        FRAG_LENGTH_s12="$FRAG_LENGTH_s11"            # used for -fragLength <# | auto | given> option; default: auto; "none" to use default or should set to "$FRAG_LENGTH_s11" to match stage 11; may have to use given for PE RNAseq to prevent extension past splice sites (will give crisp borders at exons but not plot 3' parts of reads after junctions)
        REF=hg19
        TAGS_PER_s12="$TAGS_PER_s11"                 # used for -tbp <#> option; "all" to use default: no limit; if not filtering, should be set to "$TAGS_PER_s11" to match what was used for stage 11
        HOMER_TAG_DIRNAME="$HOMER_DIRNAME"/"$THIS_SAMPLE_NAME"_TagDirectory_frag-"$FRAG_LENGTH_s11"_tbp-"$TAGS_PER_s11"
        NORM_s12=1e6                    # used for -norm <#> option
        RES_s12=1                       # used for -res <#>; resoultion in bp
        ## -fsize <#>                               # hardcoded in script for now
        # -normLength <#>                           # not implemented
        ## -strand <both/separate>                  # using switch based on STRAND_TYPE
        # Output format
            # use switch for bedgraph (default for HOMER) vs bigwig     # not currently implemented


##### DO NOT EDIT PAST HERE FOR NORMAL USAGE #####

blue="\033[0;36m"
green="\033[0;32m"
red="\033[0;31m"
NC="\033[0m"    # no color (turn off color)

### Functions for error checking etc ###

function getJobID {
    # Gets job ID from submit script log file - but only after sbatch -W finishes
    # Probably could get from STAGE_OUTPUT file before job exits
    # Usage: getJobID <$JOBNAME>
    JOB_ID="$(grep -A 3 "Running $1" "$SUBMIT_LOG" | grep "Submitted batch job" | awk '{print $4}')" # This will need to be stage- and sample-specific
}

function slurmErrorCheck {
    # This looks at .err file after sbatch job has completed or failed to check for errors that do not pass non-zero exit codes to pipeline
    # Usage: slurmErrorCheck <$STAGE_ERROR> <$JOB_ID>
    if [ $(grep -c "slurmstepd: error:" "$1"."$2".*.err) -gt 0 ]
        then
            echo -e "${red}slurmstepd ERROR -check "$1"."$2".*.err ${NC}"
            grep "slurmstepd: error:" "$1"."$2".*.err
            OUTPUT_STATUS=1
    fi
}

function slurmExitCodeCheck {
    # This uses sacct to check exit codes for the job(s); there are 2 codes separated by ":" for the original job (called by called by sbatch --wrap inside pipeline script, the job.batch (called by sbatch --wrap inside pipeline script), and the job.0 (called by srun inside stage script)
    # Usage: slurmExitCodeCheck <$JOB_ID>
    if [ $(sacct -p -n -j $1 | awk -F'[|:]' '{print $7}' | grep -c -v "0") -gt 0 ] || [ $(sacct -p -n -j $1 | awk -F'[|:]' '{print $8}' | grep -c -v "0") -gt 0 ]
        then
            echo -e "${red}ERROR - non-zero exit code for job or job step:${NC}"
            sacct -j $1
            OUTPUT_STATUS=1
    fi
}

function copyOutErrLog {
    # Copy standard output and standard error to logfile
    # Usage: copyOutErrLog <Title> <$STAGE_ERROR> <$JOB_ID>
    echo -e "--------------------\nStage "$1"\n--------------------" >> "$LOGFILE"
    for i in $(ls "$2"."$3".*.out)
    do
        echo -ne "OUTPUT FILE: "$i"" >> "$LOGFILE"
        cat $i >> "$LOGFILE"
        rm $i
    done

    for i in $(ls "$2"."$3".*.err)
    do
        echo -e "\nERROR FILE: "$i"" >> "$LOGFILE"
        cat "$i" >> "$LOGFILE"
        rm "$i"
        echo -e "\n" >> "$LOGFILE"
    done
}

function checkOutputStatus {
    # Checks $OUTPUT_STATUS from sbatch command and exits if non-zero
    # Usage: checkOutputStatus <MESSAGE or $STAGE_NAME>
    if [ "$OUTPUT_STATUS" -ne 0 ]
        then
            echo -e "${red}Stage "$1" failed!!${NC}"
            OUTPUT_STATUS=""
            exit 1
    fi
}
###


echo -e "----------\n${blue}Starting "$PIPELINE_TITLE" v"$PIPELINE_VERSION" stage(s) "$START_AT_STAGE" to "$END_AT_STAGE" for "$THIS_SAMPLE_NAME"${NC}\n----------"

# Check for Sample FASTQ file(s) and set up FASTQR1 and/or FASTQR2 variables for PE/SE data
FASTQR1_FILE="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R1.fastq.gz 

if [ ! -f "$FASTQR1_FILE" ]
then
    echo -e "${red}FASTQR1 file does not exist: "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R1.fastq.gz
    "
    exit 1
else
    echo "FASTQR1 file found: "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R1.fastq.gz
    "
fi

if [ $SEQ_TYPE = "SE" ]
then
    echo "Running "$PIPELINE_TYPE" pipeline in single-end (SE) mode
    "
    FASTQR2_FILE="none"  # Set to "none" rather than empty to avoid sending empty variable (eg to FASTQ_MCF / FASTQ_SCREEN / BOWTIE2) # Should not matter if using $SEQ_TYPE switch

elif [ $SEQ_TYPE = "PE" ]
then
    echo "Running "$PIPELINE_TYPE" pipeline in paired-end (PE) mode
    "
    FASTQR2_FILE="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R2.fastq.gz

    if [ ! -f "$FASTQR2_FILE" ]
    then
        echo -e "${red}FASTQR2 file does not exist: "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R2.fastq.gz
        "
        exit 1
    else
        echo "FASTQR2 file found: "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Raw/"$THIS_SAMPLE_NAME"_R2.fastq.gz
        "
    fi
fi

##### SETTING UP DIRECTORIES #####
# Create std_out_err directory
if [ ! -d "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/ ]
then
        echo "making std_out_err directory "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/
        "
        mkdir "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/
fi

### These directory creation steps should be moved to appropriate stages to avoid making them before required
## make a ProjectCounts directory inside the analysis directory
## uncomment for pipelines requiring counts
# if [ ! -d "$THIS_ANALYSIS_DIR"/ProjectCounts/ ]
# then
#         mkdir "$THIS_ANALYSIS_DIR"/ProjectCounts/
# fi

## make a DifferentialExpression directory inside the analysis directory
## uncomment for pipelines requiring differential expression analysis
# if [ ! -d "$THIS_ANALYSIS_DIR"/DifferentialExpression/ ]
# then
#         mkdir "$THIS_ANALYSIS_DIR"/DifferentialExpression/
# fi

## copy the samples.txt file inside the scripts directory
## uncomment for pipelines requiring counts
#cp -r "$THIS_ANALYSIS_DIR"/samples.txt "$THIS_ANALYSIS_DIR"/scripts/
# make countsFilesList.txt inside the scripts directory because we need this for making the counts files
#for i in `cat samples.txt`; do echo -e ""$i"\t"$THIS_ANALYSIS_DIR"/Sample_"$i"/Counts/"$i".0mismatchHighQual.counts.tab" > "$THIS_ANALYSIS_DIR"/scripts/countsFilesList.txt

##### Capture snapshot of stage scripts to be used for this pipeline run #####
# This will ensure that old versions are preserved in case of major changes
SCRIPTS_BAK_DIR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/stageScripts_bak/"$PIPELINE_TYPE"-"$(basename "$THIS_ANALYSIS_DIR")"-Stage"$START_AT_STAGE"-"$END_AT_STAGE"-"$THIS_SAMPLE_NAME"-"$(date "+%Y-%m-%d-%H%M")"
mkdir --parents "$SCRIPTS_BAK_DIR"
for i in $(seq ${START_AT_STAGE} 1 ${END_AT_STAGE})
do
    cp "$PIPELINE_SCRIPTS_DIR"/${i}_*.sh  "$SCRIPTS_BAK_DIR"
done

##### PIPELINE LOGFILE #####
LOGFILE="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$PIPELINE_TYPE"-"$(basename "$THIS_ANALYSIS_DIR")"-Stage"$START_AT_STAGE"-"$END_AT_STAGE"-"$THIS_SAMPLE_NAME"-"$(date "+%Y-%m-%d-%H%M")".log
echo "
#######################################
"$PIPELINE_TITLE" v"$PIPELINE_VERSION"
Stage(s): "$START_AT_STAGE"-"$END_AT_STAGE"
Run: $(date)
Sample: "$THIS_SAMPLE_NAME"
#######################################
COMMENTS:
"$COMMENTS"

GLOBAL SETTINGS:
From command line (via analysis_setup.sh):
Sample name: "$THIS_SAMPLE_NAME"
Analysis directory: "$THIS_ANALYSIS_DIR"
User account: "$THIS_USER_ACCOUNT"
Begin at stage: "$START_AT_STAGE"
Finish at stage: "$END_AT_STAGE"
Set within "$PIPELINE_TITLE"v"$PIPELINE_VERSION".sh
Sequencing type: "$SEQ_TYPE"
Strand type: "$STRAND_TYPE"
Read length: "$READ_LENGTH"
Pipeline scripts directory: "$PIPELINE_SCRIPTS_DIR"
QC directory: "$QC_DIR_NAME"
Tracks directory: "$TRACKS_DIRNAME"
Read 1 fastq file: "$FASTQR1_FILE"
Read 2 fastq file: "$FASTQR2_FILE"
" > "$LOGFILE"
# ADD some global report params? Comments?

##### PIPELINE STAGES #####

# 0) PRE_FASTQC
        # run the stage
        STAGE_NAME="0-PRE_FASTQ"
        
        if [ $START_AT_STAGE -eq 0 ]
        then
                echo -e "${blue}Starting stage "$STAGE_NAME" for "$THIS_SAMPLE_NAME"${NC}
                "

                if [ ! -d "$QC_DIR_NAME" ]
                then
                      echo "making QC directory: "$QC_DIR_NAME"
                      "
                      mkdir --parents "$QC_DIR_NAME"
                fi

                #sh $PIPELINE_SCRIPTS_DIR/0_PRE_FASTQC.sh
                #Usage: 0_PRE_FASTQC.sh <SAMPLE_DIR> <SAMPLE_NAME> <FASTQR1_FILE/FASTQR2_FILE> <QC_DIR_NAME> <OUT_DIR_NAME> <THREADS>  

                # Read 1
                JOB_NAME=stage-"$STAGE_NAME"_R1-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=32G \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/0_PRE_FASTQC.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$FASTQR1_FILE" "$QC_DIR_NAME" FASTQC_pre_filtered 8\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME"_read1 "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"_read1
                

                # Check for PE and run Read 2
                if [ $SEQ_TYPE = "PE" ]
                then
                    JOB_NAME=stage-"$STAGE_NAME"_R2-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                    STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                    STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                    echo -e "${blue}Running "$JOB_NAME"${NC}
                    "
                    sbatch -W \
                            --account="$THIS_USER_ACCOUNT" \
                            --job-name="$JOB_NAME" \
                            --output="$STAGE_OUTPUT".%j.%N.out \
                            --error="$STAGE_ERROR".%j.%N.err \
                            --partition=defq \
                            --time=10:00:00 \
                            --mem=32G \
                            --nodes=1 \
                            --cpus-per-task=8 \
                            --ntasks=1 \
                            --wrap="\
                                    sh "$PIPELINE_SCRIPTS_DIR"/0_PRE_FASTQC.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$FASTQR2_FILE" "$QC_DIR_NAME" FASTQC_pre_filtered 8\
                                    "

                    # Catch output status
                    OUTPUT_STATUS=$?

                    # Get Job ID
                    getJobID "$JOB_NAME"

                    # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                    slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                    # Check for exit code error: <function> <$JOB_ID>
                    slurmExitCodeCheck "$JOB_ID"
                    
                    # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                    copyOutErrLog "$STAGE_NAME"_read2 "$STAGE_ERROR" "$JOB_ID"

                    # check output status: <function> <stage label>
                    checkOutputStatus "$STAGE_NAME"_read2
                
                fi
                
                echo -e "${green}Stage "$STAGE_NAME" complete for "$THIS_SAMPLE_NAME"${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 1) FASTQ-MCF
        # run the stage
        STAGE_NAME="1-FASTQ-MCF"
        if [ $START_AT_STAGE -eq 1 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                # make a Processed directory inside the sample
                if [ ! -d "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/ ]
                then
                        mkdir "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/
                fi

                #sh $PIPELINE_SCRIPTS_DIR/1_FASTQ_MCF.sh
                #Usage: 1_FASTQ_MCF.sh <SEQ_TYPE> <READ_LENGTH> <SAMPLE_DIR> <SAMPLE_NAME> <FASTQR1_FILE> <FASTQR2_FILE> <QC_DIR_NAME> <MIN_QUAL> <MIN_SEQ_LENGTH> <MIN_ADAPTER> <MIN_PERC_OCCUR> <MAX_PERC_DIFF> <CONTAMINANTS_FASTA>                                      

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=70G \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/1_FASTQ_MCF_BB.sh \
                                "$SEQ_TYPE" \
                                "$READ_LENGTH" \
                                "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ \
                                "$THIS_SAMPLE_NAME" \
                                "$FASTQR1_FILE" \
                                "$FASTQR2_FILE" \
                                "$QC_DIR_NAME" \
                                "$MIN_QUAL" \
                                "$MIN_SEQ_LENGTH" \
                                "$MIN_ADAPTER" \
                                "$MIN_PERC_OCCUR" \
                                "$MAX_PERC_DIFF" \
                                "$CONTAMINANTS_FASTA"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi
        
        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 2) POST_FASTQC 
        # run the stage
        STAGE_NAME="2-POST_FASTQC"
        if [ $START_AT_STAGE -eq 2 ]
        then
                echo -e "${blue}Starting stage "$STAGE_NAME" for "$THIS_SAMPLE_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/2_POST_FASTQC.sh
                #Usage: 2_POST_FASTQC.sh <SAMPLE_DIR> <SAMPLE_NAME> <FASTQ_FILE_NAME> <QC_DIR_NAME> <OUT_DIR_NAME> <THREADS>

                # Read 1
                JOB_NAME=stage-"$STAGE_NAME"_R1-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=32G \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/2_POST_FASTQC.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR1_FILE")" "$QC_DIR_NAME" FASTQC_post_filtered 8
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME"_read1 "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"_read1


                # Check for PE and run Read 2
                if [ $SEQ_TYPE = "PE" ]
                then
                    JOB_NAME=stage-"$STAGE_NAME"_R2-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                    STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                    STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                    echo -e "${blue}Running "$JOB_NAME"${NC}
                    "
                    sbatch -W \
                            --account="$THIS_USER_ACCOUNT" \
                            --job-name="$JOB_NAME" \
                            --output="$STAGE_OUTPUT".%j.%N.out \
                            --error="$STAGE_ERROR".%j.%N.err \
                            --partition=defq \
                            --time=10:00:00 \
                            --mem=32G \
                            --nodes=1 \
                            --cpus-per-task=8 \
                            --ntasks=1 \
                            --wrap="\
                                    sh "$PIPELINE_SCRIPTS_DIR"/2_POST_FASTQC.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR2_FILE")" "$QC_DIR_NAME" FASTQC_post_filtered 8\
                                    "

                    # Catch output status
                    OUTPUT_STATUS=$?

                    # Get Job ID
                    getJobID "$JOB_NAME"

                    # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                    slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                    # Check for exit code error: <function> <$JOB_ID>
                    slurmExitCodeCheck "$JOB_ID"
                    
                    # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                    copyOutErrLog "$STAGE_NAME"_read2 "$STAGE_ERROR" "$JOB_ID"

                    # check output status: <function> <stage label>
                    checkOutputStatus "$STAGE_NAME"_read2

                fi

                echo -e "${green}Stage "$STAGE_NAME" complete for "$THIS_SAMPLE_NAME"${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi
        
        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 3) FASTQ SCREEN
        # run the stage
        STAGE_NAME="3-FASTQ_SCREEN"
        if [ $START_AT_STAGE -eq 3 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/3_FASTQ_SCREEN.sh
                #Usage: 3_FASTQ_SCREEN.sh <SEQ_TYPE> <SAMPLE_DIR> <SAMPLE_NAME> <FASTQR1_FILE> <FASTQR2_FILE> <QC_DIR_NAME> <THREADS>
                               
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=16G \
                        --nodes=1 \
                        --cpus-per-task=4 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/3_FASTQ_SCREEN.sh "$SEQ_TYPE" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR1_FILE")" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR2_FILE")" "$QC_DIR_NAME" 4\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi
        
        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 4) MAPPING
        # run the stage
        STAGE_NAME="4-TOPHAT2-ALIGN"
        if [ $START_AT_STAGE -eq 4 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                # switch to select correct index and GTF
                if [ $SPIKE-IN = none ]
                then
                    INDEX="$BOWTIE_INDEX"
                    GTF="$TOPHAT_GTF"
                    TX_INDEX="$TRANSCRIPTOME_INDEX"
                elif [ $SPIKE-IN = both ]
                then
                    INDEX="$BOWTIE_INDEX_BOTH"
                    GTF="$TOPHAT_GTF_BOTH"
                    TX_INDEX="$TRANSCRIPTOME_INDEX_BOTH"
                elif [ $SPIKE-IN = dm ]
                then
                    INDEX="$BOWTIE_INDEX_DM6"
                    GTF="$TOPHAT_GTF_DM6"
                    TX_INDEX="$TRANSCRIPTOME_INDEX_DM6"
                elif [ $SPIKE-IN = ercc ]
                then
                    INDEX="$BOWTIE_INDEX_ERCC"
                    GTF="$TOPHAT_GTF"
                    TX_INDEX="$TRANSCRIPTOME_INDEX"
                fi

                #sh $PIPELINE_SCRIPTS_DIR/4_TOPHAT2_ALIGN.sh
                #Usage: 4_TOPHAT2_ALIGN.sh <SEQ_TYPE> <SAMPLE_DIR> <SAMPLE_NAME> <FASTQR1_FILE> <FASTQR2_FILE> <INDEX> <GTF> <TX_INDEX> <STRAND_TYPE> <MATE_INNER_DIST> <MATE_STD_DEV> <BAM_OUT_FILENAME> <OUT_DIR_NAME> <THREADS>
                                           
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=bigmem \
                        --time=10:00:00 \
                        --mem=400G \
                        --nodes=1 \
                        --cpus-per-task=12 \
                        --ntasks=1 \
                        --wrap="\
                                sh $PIPELINE_SCRIPTS_DIR/4_TOPHAT2_ALIGN.sh "$SEQ_TYPE" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR1_FILE")" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Processed/trimmed_"$(basename "$FASTQR2_FILE")" "$INDEX" "$GTF" "$TX_INDEX" "$STRAND_TYPE" "$MATE_INNER_DIST" "$MATE_STD_DEV" "$THIS_SAMPLE_NAME".mapped.no-rgid.bam Alignments 12\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi
        
        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 5) ADD RGID
        # run the stage
        STAGE_NAME="5-ADD_RGID"
        if [ $START_AT_STAGE -eq 5 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/5_ADD_RGID.sh
                #Usage: 5_ADD_RGID.sh <SAMPLE_NAME> <PLATFORM> <DATE> <PI (investigator code)> <LIBRARY (PE/SE)> <SEQ_CORE> <SEQ_ID> <EXPERIMENT> <SAMPLE_DIR> <ALIGNMENT_DIRNAME> <BAM_IN_FILENAME> <BAM_OUT_FILENAME> <PICARD_MEM>
                              
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=8G \
                        --nodes=1 \
                        --cpus-per-task=1 \
                        --ntasks=1 \
                        --wrap="\
                                sh $PIPELINE_SCRIPTS_DIR/5_ADD_RGID.sh "$THIS_SAMPLE_NAME" "$PLATFORM" "$DATE" "$PI" "$LIBRARY" "$SEQ_CORE" "$SEQ_ID" "$EXPERIMENT" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ Alignments "$THIS_SAMPLE_NAME".mapped.no-rgid.bam "$THIS_SAMPLE_NAME".mapped.rgid.bam "$PICARD_MEM_5"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 6) MAPQ FILTER
        # run the stage
        STAGE_NAME="6-MAPQ_FILTER"
        if [ $START_AT_STAGE -eq 6 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/6_MAPQ_FILTER.sh
                #Usage: 6_MAPQ_FILTER.sh <SAMPLE_DIR> <ALIGNMENT_DIRNAME> <BAM_IN_FILENAME> <BAM_OUT_FILENAME> <MIN_MAPQ>
                                
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=8G \
                        --nodes=1 \
                        --cpus-per-task=1 \
                        --ntasks=1 \
                        --wrap="\
                                sh $PIPELINE_SCRIPTS_DIR/6_MAPQ_FILTER.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ Alignments "$THIS_SAMPLE_NAME".mapped.rgid.bam "$THIS_SAMPLE_NAME".mapped.rgid.filtered.bam "$MIN_MAPQ"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 7) SORT BAM
        # run the stage
        STAGE_NAME="7-SORT_BAM"
        if [ $START_AT_STAGE -eq 7 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/7_SORT_BAM.sh
                #Usage: 7_SORT_BAM.sh <SAMPLE_DIR> <ALIGNMENT_DIRNAME> <BAM_IN_FILENAME> <BAM_OUT_FILENAME> <PICARD_MEM>
                
                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=32G \
                        --nodes=1 \
                        --cpus-per-task=6 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/7_SORT_BAM.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ Alignments "$THIS_SAMPLE_NAME".mapped.rgid.filtered.bam "$THIS_SAMPLE_NAME".mapped.rgid.filtered.sorted.bam "$PICARD_MEM_7"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 8) MARK DUPLICATES
        # run the stage
        STAGE_NAME="8-MARK_DUPLICATES"
        if [ $START_AT_STAGE -eq 8 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                #sh $PIPELINE_SCRIPTS_DIR/8_MARK_DUPLICATES.sh 
                #Usage: 8_MARK_DUPLICATES.sh <SAMPLE_DIR> <ALIGNMENT_DIRNAME> <BAM_IN_FILENAME> <BAM_OUT_FILENAME> <PICARD_MEM> <mark/remove duplicates>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=120G \
                        --nodes=1 \
                        --cpus-per-task=6 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/8_MARK_DUPLICATES.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ Alignments "$THIS_SAMPLE_NAME".mapped.rgid.filtered.sorted.bam "$THIS_SAMPLE_NAME".mapped.rgid.filtered.sorted.dups_mark.bam "$PICARD_MEM_8" "$DUPLICATES"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 9) ALIGNMENT METRICS
        # run the stage
        STAGE_NAME="9-ALIGNMENT_METRICS"
        if [ $START_AT_STAGE -eq 9 ]

        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                if [ "$SEQ_TYPE" = "PE" ]
                    then
                        echo "Collecting paired-end alignment metrics"
                        ALIGNMENT_METRICS_SCRIPT="9_PE_ALIGNMENT_METRICS.sh"
                elif [ "$SEQ_TYPE" = "SE" ]
                    then
                        echo "Collecting single-end alignment metrics"
                        ALIGNMENT_METRICS_SCRIPT="9_SE_ALIGNMENT_METRICS.sh"
                else
                    echo -e "${red}ERROR - SEQ_TYPE parameter not recognised:${NC} "$SEQ_TYPE"
                    "
                    exit 1
                fi

                # sh "$PIPELINE_SCRIPTS_DIR"/9_PE_ALIGNMENT_METRICS.sh
                # Usage: 9_PE_ALIGNMENT_METRICS.sh <ANALYSIS_DIR> <QC_DIR> <SAMPLE_DIR> <SAMPLE_NAME> <PICARD_MEM> <REF_FILE>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=70G \
                        --nodes=1 \
                        --cpus-per-task=4 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/"$ALIGNMENT_METRICS_SCRIPT" "$THIS_ANALYSIS_DIR" "$QC_DIR_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$PICARD_MEM_9" "$REF_FILE"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 10) RSEQC
        # run the stage
        STAGE_NAME="10-RSEQC"
        if [ $START_AT_STAGE -eq 10 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "
                
                # sh 10_RSEQC.sh
                # Usage: 10_RSEQC.sh <SEQ_TYPE> <ANALYSIS_DIR> <QC_DIR> <SAMPLE_DIR> <SAMPLE_NAME> <REFSEQ_BED> <HOUSEKEEPING_BED>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=100GB \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/10_RSEQC.sh "$SEQ_TYPE" "$THIS_ANALYSIS_DIR" "$QC_DIR_NAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$REFSEQ_BED" "$HOUSEKEEPING_BED"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi


# 11) HTSEQ COUNT
        # run the stage
        STAGE_NAME="11-HTSEQ-COUNT"
        if [ $START_AT_STAGE -eq 11 ]
        then
                JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"
                echo -e "${blue}Running "$JOB_NAME"${NC}
                "

                # sh 11_HTSEQ_COUNT.sh
                # Usage: 11_HTSEQ_COUNT.sh <ANALYSIS_DIR> <SAMPLE_DIR> <SAMPLE_NAME> <STRAND_TYPE> <SPIKE_IN> <SORT_ORDER> <GTF> <DM_GTF> <ERCC_GTF> <FEATURETYPE> <IDATTR> <MINAQUAL> <MODE> <COUNTS_DIRNAME>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name="$JOB_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --partition=defq \
                        --time=10:00:00 \
                        --mem=120GB \
                        --nodes=1 \
                        --cpus-per-task=16 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/11_HTSEQ_COUNT.sh "$THIS_ANALYSIS_DIR" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$THIS_SAMPLE_NAME" "$STRAND_TYPE" "$SPIKE_IN" "$ORDER" "$GTF" "$DM_GTF" "$ERCC_GTF" "$FEATURETYPE" "$IDATTR" "$HT_MINAQUAL" "$MODE" "$COUNTS_DIRNAME"\
                                "

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi

# 12) HOMER Make Tag directory
        # run the stage
        STAGE_NAME="12-HOMER_MAKE_TAG_DIRECTORY"
        if [ $START_AT_STAGE -eq 12 ]

        then
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$PIPELINE_TYPE"-stage"$STAGE_NAME"-"$THIS_SAMPLE_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$PIPELINE_TYPE"-stage"$STAGE_NAME"-"$THIS_SAMPLE_NAME"
                echo -e "${blue}Starting stage "$STAGE_NAME" for "$THIS_SAMPLE_NAME"${NC}
                "

                # sh "$PIPELINE_SCRIPTS_DIR"/12_HOMER_BAM_makeTagDirectory.sh
                # Usage: 12_HOMER_BAM_makeTagDirectory.sh <SAMPLE_DIR> <HOMER_DIRNAME> <BAM_IN_FILENAME> <SAMPLE_NAME> <STRAND_TYPE (strand-specific-fwd/strand-specific-rev/unstranded)> <SEQ_TYPE (PE/SE)> <REF> <FRAG_LENGTH> <TAGS_PER>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name=stage"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --time=24:00:00 \
                        --mem=108G \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                  sh "$PIPELINE_SCRIPTS_DIR"/12_HOMER_MAKE_TAG_DIR.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$HOMER_DIRNAME" "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/Alignments/"$THIS_SAMPLE_NAME".mapped.rgid.filtered.sorted.dups_mark.bam "$THIS_SAMPLE_NAME" "$STRAND_TYPE" "$SEQ_TYPE" "$REF" "$FRAG_LENGTH_s11" "$TAGS_PER_s11""

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi

# 13) HOMER make UCSCfile
        # run the stage
        STAGE_NAME="13-HOMER_MAKE_UCSC_FILE"
        if [ $START_AT_STAGE -eq 13 ]

        then
                STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$PIPELINE_TYPE"-stage"$STAGE_NAME"-"$THIS_SAMPLE_NAME"
                STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$PIPELINE_TYPE"-stage"$STAGE_NAME"-"$THIS_SAMPLE_NAME"
                echo -e "${blue}Starting stage "$STAGE_NAME" for "$THIS_SAMPLE_NAME"${NC}
                "

                # sh "$PIPELINE_SCRIPTS_DIR"/13_HOMER_MAKE_UCSC_FILE.sh
                # Usage: 13_HOMER_MAKE_UCSC_FILE.sh <SAMPLE_DIR> <HOMER_TAG_DIRNAME> <SAMPLE_NAME> <TRACKS_DIRNAME> <STRAND_TYPE (strand-specific-fwd/strand-specific-rev/unstranded)> <REF> <FRAG_LENGTH> <TAGS_PER> <NORM> <RES>

                sbatch -W \
                        --account="$THIS_USER_ACCOUNT" \
                        --job-name=stage"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME" \
                        --output="$STAGE_OUTPUT".%j.%N.out \
                        --error="$STAGE_ERROR".%j.%N.err \
                        --time=24:00:00 \
                        --mem=108G \
                        --nodes=1 \
                        --cpus-per-task=8 \
                        --ntasks=1 \
                        --wrap="\
                                sh "$PIPELINE_SCRIPTS_DIR"/13_HOMER_MAKE_UCSC_FILE.sh "$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/ "$HOMER_TAG_DIRNAME" "$THIS_SAMPLE_NAME" "$TRACKS_DIRNAME" "$STRAND_TYPE" "$REF" "$FRAG_LENGTH_s12" "$TAGS_PER_s12" "$NORM_s12" "$RES_s12""

                # Catch output status
                OUTPUT_STATUS=$?

                # Get Job ID
                getJobID "$JOB_NAME"

                # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
                slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"

                # Check for exit code error: <function> <$JOB_ID>
                slurmExitCodeCheck "$JOB_ID"
                
                # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
                copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"

                # check output status: <function> <stage label>
                checkOutputStatus "$STAGE_NAME"
       
                echo -e "${green}"$JOB_NAME" complete.${NC}
                "
                # update the stage
                START_AT_STAGE=$(($START_AT_STAGE+1))
                STAGE_NAME=""
                STAGE_OUTPUT=""
                STAGE_ERROR=""
        fi

        # check if we need to exit
        if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
        then
                echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
                exit 0
        fi

##########################################
# # NN) Stage Template    ## (some stages might require detection of SE/PE etc eg see FASTQC stages)  ## UNCOMMENT WHOLE SECTION AND REMOVE ALL COMMENTS WITH "##" BEFORE USING
#         # run the stage
#         STAGE_NAME="NN-STAGE-NAME"      ## Change NN to suit
#         if [ $START_AT_STAGE -eq NN ]   ## Change NN to suit
#         then
#                 JOB_NAME=stage-"$STAGE_NAME"-"$PIPELINE_TYPE"-"$THIS_SAMPLE_NAME"    ## This is used by several other parts -can add extra labels in here eg "_R1"
#                 STAGE_OUTPUT="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"       ## Do not change: this is used for creation of log file and cleanup
#                 STAGE_ERROR="$THIS_ANALYSIS_DIR"/Sample_"$THIS_SAMPLE_NAME"/std_out_err/"$JOB_NAME"       ## Do not change: this is used for creation of log file and cleanup
#                 echo -e "${blue}Running "$JOB_NAME"${NC}
#                 "   ## Do not change: This echo line is used for finding job ID
                
#                 # sh NN_STAGE_NAME.sh
#                 # Usage: NN_STAGE_NAME.sh <ARG1> <ARG2> <ARG3> <ARG4> <etc...>  ## ARGS will use global and/or stage-specific variables entered in section at top of this pipeline script
#                                                                                 ## Any variables specific to this stage should be added to editable variables section at top to facilitate easy customization

#                 sbatch -W \     ## -W forces sbatch to wait for command inside --wrap to finish
#                         --account="$THIS_USER_ACCOUNT" \       ## Do not change
#                         --job-name="$JOB_NAME" \       ## Do not change
#                         --output="$STAGE_OUTPUT".%j.%N.out \       ## Do not change: this is used for creation of log file and cleanup
#                         --error="$STAGE_ERROR".%j.%N.err \       ## Do not change: this is used for creation of log file and cleanup
#                         --partition=defq \  ## Can use bigmem for some stages
#                         --time=10:00:00 \    ## Alter to suit
#                         --mem=32GB \    ## Alter to suit
#                         --nodes=1 \       ## Do not change
#                         --cpus-per-task=8 \     ## Alter to suit
#                         --ntasks=1 \       ## Do not change
#                         --wrap="\
#                                 sh "$PIPELINE_SCRIPTS_DIR"/NN_STAGE_NAME.sh <ARG1> <ARG2> <ARG3> <ARG4> <etc...>"   ## --wrap sends contents as shell script to slurm. This is where the actual stage script is called and ARGS are passed to be used as variables. Can be broken onto separate lines with "\" for ease of reading but be careful of formatting.

#                 ## Do not remove the following lines as they are used to catch errors and compile log files
#                 # Catch output status
#                 OUTPUT_STATUS=$?       ## Do not change

#                 # Get Job ID
#                 getJobID "$JOB_NAME"       ## Do not change

#                 # Check for SLURM error: <function> <$STAGE_ERROR> <$JOB_ID>
#                 slurmErrorCheck "$STAGE_ERROR" "$JOB_ID"       ## Do not change

#                 # Check for exit code error: <function> <$JOB_ID>
#                 slurmExitCodeCheck "$JOB_ID"       ## Do not change
                
#                 # Copy standard output and standard error to logfile: <function> <Title> <$STAGE_ERROR> <$JOB_ID>
#                 copyOutErrLog "$STAGE_NAME" "$STAGE_ERROR" "$JOB_ID"       ## First ARG <Title> can have extra text added eg "_R1" if needed; otherwise leave as $STAGE_NAME

#                 # check output status: <function> <stage label>       ## First ARG <stage label> can have extra text added eg "_R1" if needed; otherwise leave as $STAGE_NAME
#                 checkOutputStatus "$STAGE_NAME"
       
#                 echo -e "${green}"$JOB_NAME" complete.${NC}\n"       ## Do not change
#                 # update the stage
#                 START_AT_STAGE=$(($START_AT_STAGE+1))       ## Do not change
#                 STAGE_NAME=""       ## Do not change
#                 STAGE_OUTPUT=""       ## Do not change
#                 STAGE_ERROR=""       ## Do not change
#         fi

#         # check if we need to exit       ## Do not change
#         if [ $END_AT_STAGE -eq $(($START_AT_STAGE-1)) ]
#         then
#                 echo -e "------\n${green}"$PIPELINE_TITLE" run ending at stage "$END_AT_STAGE"${NC}\n------\n"
#                 exit 0
#         fi
##########################################





# Remove unneeded large files ie BAMs


# ADD ERCC SEQs mapping

# check output status
if [ $? -ne 0 ]
then
        echo -e "     ----------\n${green}All "$PIPELINE_TYPE" stages run and completed for sample: "$THIS_SAMPLE_NAME"\n----------"
fi

exit
```

**analysis_setup.sh**

Here is the current analysis setup script at the time of writing, **`analysis_setup_v0.7.sh`**:

```{bash}
#!/bin/bash

SCRIPT_TITLE=analysis_setup.sh
SCRIPT_VERSION=0.7
# DATE: 08-27-2018
# AUTHOR: Matthew Galbraith / Santosh Khanal
# SUMMARY: 
# This script is designed to be run from Project/analysis_date/ and will generate:
# /SAMPLE/Raw directories with symbolic links to fastq files
# and
# /scripts/submitAll.sh with one command per sample in the form:
# nohup sh PATH/TO/PIPELINE_SCRIPT.sh SAMPLE_NAME ANALYSIS_DIR/ USER_NAME START_AT_STAGE END_AT_STAGE &> PIPELINE_SCRIPT.sh.SAMPLE_NAME.log &
# This script uses read1 / read2 in the third column of sample_locations.txt to deal with paired-end vs. single-end data
#
# Version 0.5: Generates new form of submit scripts that run as sbatch rather than shell jobs (should improve robustness as SLURM rather than login node will be manging pipeline script).
# MUST be used with corresponding version of pipeline script with stages called via sbatch with -W and --wrap options.
# Version 0.6: Adds SUBMIT_LOG arg passed to pipeline script for getting JOB_ID and error checking
# Version 0.7: added SBATCH --account for pipeline wrapper script; added SBATCH -p longrun and SBATCH --time=100:00:00 to prevent pipeline script from timing out (especially while waiting for resources)
#
# The submitAll.sh script should now be run from the appropriate Project/analysis_date/scripts directory as follows:
# sbatch -o submitAll.sh.log submitAll.sh


##### ARGUMENTS PASSED FROM COMMAND LINE - DO NOT EDIT THIS SECTION #####

# variables from command line:
PIPELINE_SCRIPT=${1}            # Path to pipeline scripts directory e.g TTseq_pipeline_v0.5.sh
ACCOUNT=${2}
START_AT_STAGE=${3}
END_AT_STAGE=${4}
# Other variables:
THIS_USER_ACCOUNT="$(whoami)"
#if adding any new ARGS, remember to add below to output for submitAll.sh
# TO DO: add ARG for espinosalab / HTP

blue="\033[0;36m"
green="\033[0;32m"
red="\033[0;31m"
NC="\033[0m"    # no color

EXPECTED_ARGS=4
# check if correct number of arguments are supplied from command line
if [ $# -ne $EXPECTED_ARGS ]
then
        echo -ne "${red}ERROR - expecting "$EXPECTED_ARGS" ARGS but "$#" provided:${NC}
        "$@""
        echo "Usage: sh <PATH/TO/"$SCRIPT_TITLE"> <PATH/TO/PIPELINE_SCRIPT> <ACCOUNT: ESP/HTP> <START_AT_STAGE> <END_AT_STAGE>"
        echo -e "EXAMPLE: sh /gpfs/jespinosa/shared/Matt/PipeLinesScripts/analysis_setup.sh /gpfs/jespinosa/shared/Matt/PipeLinesScripts/TTseq/TTseq_pipeline_v0.1.sh ESP 0 8"
        exit 1
fi

# Check for sample_locations.txt
if [ ! -e sample_locations.txt ]
	then
		echo -e "${red}ERROR - sample_locations.txt not found
		"
		exit 1
fi

# Check for pipeline script
if [  ! -e "$PIPELINE_SCRIPT" ]
	then
		echo -e "${red}ERROR - Script does not exist:${NC} "$PIPELINE_SCRIPT"
		"
		exit 1
fi

# Set THIS_USER_ACCOUNT
if [ "$ACCOUNT" = ESP ]
	then
		THIS_USER_ACCOUNT=espinosalab-"$(whoami)"
elif [ "$ACCOUNT" = HTP ]
	then
		THIS_USER_ACCOUNT=htp-"$(whoami)"
else
	echo -e "${red}ERROR - Incorrect account given:${NC} use either ESP or HTP
		"
		exit 1
fi

## Set up SAMPLE directories and generate symbolic links to original raw FASTQ files
# The symbolic links allow us to use informative names without renaming or duplicating original files
# This currently requires adding a third field so sample_locations.txt with read1/read2 to deal with paired-end data (single read data will just be labelled as read1)
# New in v0.2: Will not over-write existing Sample directories or symbolic links so that analysis_setup.sh can easily be re-run to create submitAll.sh scripts for different subsets of stages.

# Create SAMPLE directory:
for i in $(cat sample_locations.txt | awk '{print $1}' | uniq)
do
	SAMPLE_NAME=$(echo "$i")
	if [ -d Sample_"$SAMPLE_NAME"/Raw ]
		then
			echo "Raw data directory for "$SAMPLE_NAME" already present - no action needed"
		else
			mkdir -p Sample_"$SAMPLE_NAME"/Raw
			echo "Making directory: $(pwd)/Sample_"$SAMPLE_NAME"/Raw"
	fi
done

# Create symbolic links for read 1:
for i in $(cat sample_locations.txt | grep -w read1 | tr "\t" ":")
do
	SAMPLE_NAME=$(echo "$i" | cut -d ":" -f1)
	FASTQ_FILE=$(echo "$i" | cut -d ":" -f2)
	# Check for FASTQ_FILE
	if [ -r "$FASTQ_FILE" ]
	then
		if [ -L Sample_"$SAMPLE_NAME"/Raw/"$SAMPLE_NAME"_R1.fastq.gz ]
		then
			echo "Symbolic link to "$SAMPLE_NAME" read 1 FASTQ file already present - no action needed"
		else
			ln -s "$FASTQ_FILE" Sample_"$SAMPLE_NAME"/Raw/"$SAMPLE_NAME"_R1.fastq.gz
		fi
	else
		echo -e "${red}ERROR - read 1 FASTQ file does not exist or is not readable:${NC} "$FASTQ_FILE"
		"
		exit 1
	fi
done

# Create symbolic links for read 2:
for i in $(cat sample_locations.txt | grep -w read2 | tr "\t" ":")
do
	SAMPLE_NAME=$(echo "$i" | cut -d ":" -f1)
	FASTQ_FILE=$(echo "$i" | cut -d ":" -f2)
	# Check for FASTQ_FILE
	if [ -r "$FASTQ_FILE" ]
	then
		if [ -L Sample_"$SAMPLE_NAME"/Raw/"$SAMPLE_NAME"_R2.fastq.gz ]		# this does not check if linking path is same
		then
			echo "Symbolic link to "$SAMPLE_NAME" read 2 FASTQ file already present - no action needed"
		else
			ln -s "$FASTQ_FILE" Sample_"$SAMPLE_NAME"/Raw/"$SAMPLE_NAME"_R2.fastq.gz
		fi
	else
		echo -e "${red}ERROR - read 2 FASTQ file does not exist or is not readable:${NC} "$FASTQ_FILE"
		"
		exit 1
	fi
done

# Check for scripts directory
if [ ! -d scripts ]
then
	echo "Making directory: $(pwd)/scripts
	"
	mkdir -p $(pwd)/scripts
fi

echo "Writing commands to $(pwd)/scripts/submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh"

# write parameters to submitAll.sh
echo "#!/bin/sh

#SBATCH -J SUBMIT_$(basename ${PIPELINE_SCRIPT%.sh})
#SBATCH -p longrun
#SBATCH --time=100:00:00
#SBATCH -o ./submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh.log
#SBATCH -D ./
#SBATCH --account="$THIS_USER_ACCOUNT"
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --ntasks=1

SCRIPT_TITLE="submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh_for_$(basename $PWD)"
SCRIPT_VERSION="$SCRIPT_VERSION"
# DATE: $(date)
# AUTHOR: ${SCRIPT_TITLE%.sh}_v${SCRIPT_VERSION}.sh
# SUMMARY:
# This is an auto-generated script that contains commands to run the specified pipeline for each sample.
# 
# Sequencing details and analysis settings should be entered/changed in top section of <Pipeline_Type>.sh before running this script
# 
# This script should be run from the appropriate Project/analysis_date/scripts directory as follows:
# sbatch submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh
###### ^^ COPY AND PASTE COMMAND ABOVE TO RUN ^^ #######
########################################################

# Arguments set from command line:
# Analysis Setup script: "$0"
# PIPELINE_SCRIPT: "$1"
# THIS_USER_ACCOUNT: "$2"
# START_AT_STAGE: "$3"
# END_AT_STAGE: "$4"

# Arguments passed to "$PIPELINE_SCRIPT":
# THIS_SAMPLE_NAME: $(cat sample_locations.txt | awk '{print $1}' | uniq | tr "\n" ";" )
# THIS_ANALYSIS_DIR: $(pwd)/
# THIS_USER_ACCOUNT: espinosalab-"$THIS_USER_ACCOUNT"
# START_AT_STAGE: "$START_AT_STAGE"
# END_AT_STAGE: "$END_AT_STAGE"
# SUBMIT_LOG: "$(pwd)/scripts/$(basename "$PIPELINE_SCRIPT").SAMPLE.Stage"$START_AT_STAGE"-"$END_AT_STAGE".log" 

" > scripts/submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh

# write commands to submitAll.sh
for i in $(cat sample_locations.txt | awk '{print $1}' | uniq) # the uniq step ensures a single command per SAMPLE for PE data (rather than 1 per fastq file as listed in sample_locations.txt) (v0.2: removed the sort to preserve sensible order)
do 
	SUBMIT_LOG="$(pwd)/scripts/$(basename "$PIPELINE_SCRIPT")."$i".Stage"$START_AT_STAGE"-"$END_AT_STAGE".log"
	echo "(nohup sh "$PIPELINE_SCRIPT" "$i" $(pwd)/ "$THIS_USER_ACCOUNT" "$START_AT_STAGE" "$END_AT_STAGE" "$SUBMIT_LOG" &> "$SUBMIT_LOG") &"
done >> scripts/submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh

# need to execute commands above as subshells: (CMD) &
# then use "wait" which waits for child processes to complete
# then gather all std outs and errs to one log file:
echo "
wait

# Intialize log file:
echo  > $(pwd)/scripts/$(basename "$PIPELINE_SCRIPT").ALL.Stage"$START_AT_STAGE"-"$END_AT_STAGE".log

# Collect individual std out and err files into one log file:" >> scripts/submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh

for i in $(cat sample_locations.txt | awk '{print $1}' | uniq)
do
	SUBMIT_LOG="$(pwd)/scripts/$(basename "$PIPELINE_SCRIPT")."$i".Stage"$START_AT_STAGE"-"$END_AT_STAGE".log"
	echo "cat "$SUBMIT_LOG" >> $(pwd)/scripts/$(basename "$PIPELINE_SCRIPT").ALL.Stage"$START_AT_STAGE"-"$END_AT_STAGE".log"
	echo "rm "$SUBMIT_LOG""
done >> scripts/submitAll_"$START_AT_STAGE"-"$END_AT_STAGE".sh

echo "Done."

exit

```

In sum, it will perform the following steps:

+ set up SAMPLE directories and generate symbolic links to original raw FASTQ files
+ write the submit_all.sh script
  + contains job scheduler commands to run the provided pipeline script
  + sets up log collection



Here is an example command which will run the analysis setup with the RNA-Seq pipeline script using the Espinosa lab account with stages 0 through 11.


```{class.source=NULL}
$ $SHARED/PipeLinesScripts/analysis_setup_v0.7.sh scripts/RNAseq_pipeline_v0.65_CD4_CD8_NK.sh ESP 0 11
```

If everything runs according to plan you should end up with a submit script for the pipeline script which will be used to submit jobs to the job scheduler (SLURM).

# Submit the job

Now you just have to submit the job! The analysis_setup script will produce a submit script under the `scripts/` folder.

Here's an example of a submit script which ran stages 0 through 3, **`submitAll_0-3.sh`**: 
```{bash}
#!/bin/sh

#SBATCH -J SUBMIT_RNAseq_pipeline_v0.6
#SBATCH -p longrun
#SBATCH --time=100:00:00
#SBATCH -o ./submitAll_0-3.sh.log
#SBATCH -D ./
#SBATCH --account=espinosalab-galbrama
#SBATCH --nodes=1
#SBATCH --cpus-per-task=1
#SBATCH --ntasks=1

SCRIPT_TITLE=submitAll_0-3.sh_for_analysis_05_06_2019
SCRIPT_VERSION=0.7
# DATE: Tue May  7 10:32:23 MDT 2019
# AUTHOR: analysis_setup_v0.7.sh
# SUMMARY:
# This is an auto-generated script that contains commands to run the specified pipeline for each sample.
# 
# Sequencing details and analysis settings should be entered/changed in top section of <Pipeline_Type>.sh before running this script
# 
# This script should be run from the appropriate Project/analysis_date/scripts directory as follows:
# sbatch submitAll_0-3.sh
###### ^^ COPY AND PASTE COMMAND ABOVE TO RUN ^^ #######
########################################################

# Arguments set from command line:
# Analysis Setup script: /gpfs/jespinosa/shared/Matt/PipeLinesScripts/analysis_setup_v0.7.sh
# PIPELINE_SCRIPT: /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh
# THIS_USER_ACCOUNT: ESP
# START_AT_STAGE: 0
# END_AT_STAGE: 3

# Arguments passed to /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh:
# THIS_SAMPLE_NAME: HCT116_WT_Norm_2;HCT116_WT_Hypox_2;HCT116_WT_Norm_3;HCT116_WT_Hypox_3;HCT116_HIF1Ako_Norm_2;HCT116_HIF1Ako_Hypox_2;HCT116_HIF1Ako_Norm_3;HCT116_HIF1Ako_Hypox_3;
# THIS_ANALYSIS_DIR: /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/
# THIS_USER_ACCOUNT: espinosalab-espinosalab-galbrama
# START_AT_STAGE: 0
# END_AT_STAGE: 3
# SUBMIT_LOG: /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.SAMPLE.Stage0-3.log 


(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_WT_Norm_2 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_2.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_2.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_WT_Hypox_2 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_2.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_2.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_WT_Norm_3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_3.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_3.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_WT_Hypox_3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_3.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_3.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_HIF1Ako_Norm_2 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_2.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_2.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_HIF1Ako_Hypox_2 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_2.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_2.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_HIF1Ako_Norm_3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_3.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_3.Stage0-3.log) &
(nohup sh /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh HCT116_HIF1Ako_Hypox_3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/ espinosalab-galbrama 0 3 /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_3.Stage0-3.log &> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_3.Stage0-3.log) &

wait

# Intialize log file:
echo  > /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log

# Collect individual std out and err files into one log file:
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_2.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_2.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_2.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_2.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_3.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Norm_3.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_3.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_WT_Hypox_3.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_2.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_2.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_2.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_2.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_3.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Norm_3.Stage0-3.log
cat /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_3.Stage0-3.log >> /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.ALL.Stage0-3.log
rm /gpfs/jespinosa/shared/Projects2/RNAseq/HCT116_Hypoxia_spikein/analysis_05_06_2019/scripts/RNAseq_pipeline_v0.6.sh.HCT116_HIF1Ako_Hypox_3.Stage0-3.log

```

In sum, it will perform the following steps:

+ contains job scheduler commands to run the provided pipeline script
  + the jobs run asynchronously, so each fastq file pair is processed with it's own resources
+ collects individual log files in to one file

At the top you'll see some parameters following `#SBATCH`. These are passed to SLURM and configure how the job will be run. To submit the this script and get the job running, do

```{class.source=NULL}
$ sbatch submitAll_0-3.sh
```

You can view information about your specifc jobs using either

```{class.source=NULL}
$ squeue -u username
```

or

```{class.source=NULL}
$ watching -u username
```

# Check the logs

Check the run logs to ensure everything ran to completion without error. There will be a log file in the scripts folder which will give information as to which stage the script failed on. If there are issues, determine at which stage the error occurred and check `std_err_out` folder within the per-sample output folders. Here you'll find more detailed tool-specific output.

As an example for things you might look for, if you search the log for the pattern `ending` you can see how many samples reached the end of processing. We had 8 samples, so things look good here.

```{class.source=NULL}
$ grep ending RNAseq_pipeline_v0.6.sh.ALL.Stage5-10.log
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
RNAseq_pipeline run ending at stage 10
$ grep ending RNAseq_pipeline_v0.6.sh.ALL.Stage5-10.log | wc -l
8
```

Finding failures will be less clear cut, but if you encounter fewer-than-expected `run ending` messages, a good place to start is searching for the pattern `failed`.

```{class.source=NULL}
$ grep failed  RNAseq_pipeline_v0.65_CD4_CD8_NK.sh.ALL.Stage5-11.log
Stage 11-HTSEQ-COUNT failed!!
Stage 11-HTSEQ-COUNT failed!!
```

# Final directory structure

At the end, you should have some similar to the following structure.

```{class.source=NULL}
├── HCT116_Hypoxia_spikein
│   ├── analysis_05_06_2019
│   │   ├── QC
│   │   ├── Sample_HCT116_HIF1Ako_Hypox_2
│   │   │   ├── Alignments
│   │   │   ├── Counts
│   │   │   ├── Processed
│   │   │   ├── Raw
│   │   │   └── std_out_err
│   │   ├── Sample_HCT116_HIF1Ako_Hypox_3
│   │   ├── Sample_HCT116_HIF1Ako_Norm_2
│   │   ├── Sample_HCT116_HIF1Ako_Norm_3
│   │   ├── Sample_HCT116_WT_Hypox_2
│   │   ├── Sample_HCT116_WT_Hypox_3
│   │   ├── Sample_HCT116_WT_Norm_2
│   │   ├── Sample_HCT116_WT_Norm_3
│   │   ├── sample_locations.txt
│   │   └── scripts
│   ├── raw_05_06_2019
│   │   ├── H202SC19030981
│   │   └── Novogene_QC
```





